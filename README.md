# MongoDB Atlas RAG System

A production-grade **Retrieval-Augmented Generation (RAG)** system demonstrating advanced LangChain integration with MongoDB Atlas Vector Search and Llama 3.2.

## ğŸ¯ Project Overview

This system implements a complete RAG pipeline that:
- Processes and chunks documents with line number preservation
- Generates embeddings using local sentence-transformers models
- Stores vectors in MongoDB Atlas with semantic search capabilities
- Retrieves contextually relevant chunks using vector similarity
- Generates accurate, source-cited answers using Llama 3.2

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Frontend Layer                        â”‚
â”‚  React + TypeScript + Tailwind CSS                          â”‚
â”‚  â€¢ File Upload Component                                    â”‚
â”‚  â€¢ Query Interface                                          â”‚
â”‚  â€¢ Response Display with Source Citations                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ HTTP/REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Backend Layer                            â”‚
â”‚  Flask + LangChain + MongoDB                                 â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Document   â”‚  â”‚  Embedding  â”‚  â”‚   Vector     â”‚       â”‚
â”‚  â”‚  Processor   â”‚â”€â”€â”‚   Service   â”‚â”€â”€â”‚    Store     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                              â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚           RAG Service (LangChain)                 â”‚       â”‚
â”‚  â”‚  â€¢ Vector Retriever                                â”‚       â”‚
â”‚  â”‚  â€¢ Context Assembly                                â”‚       â”‚
â”‚  â”‚  â€¢ LLM Chain (Llama 3.2)                          â”‚       â”‚
â”‚  â”‚  â€¢ Response Formatting                             â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MongoDB Atlas                                 â”‚
â”‚  â€¢ Vector Search Index (384 dimensions)                       â”‚
â”‚  â€¢ Cosine Similarity                                           â”‚
â”‚  â€¢ Document Chunks with Metadata                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”— RAG Chain Implementation

### Core RAG Pipeline

The RAG chain is implemented in `backend/services/rag_service.py` using LangChain's modular architecture:

```python
# 1. Document Processing & Chunking
Document â†’ Text Extraction â†’ RecursiveCharacterTextSplitter â†’ Chunks with Line Numbers

# 2. Embedding Generation
Chunks â†’ SentenceTransformer (all-MiniLM-L6-v2) â†’ 384-dimensional vectors

# 3. Vector Storage
Vectors + Metadata â†’ MongoDB Atlas Vector Store â†’ Indexed for Search

# 4. Retrieval Phase
User Query â†’ Query Embedding â†’ Vector Search â†’ Top-K Relevant Chunks

# 5. Generation Phase
Retrieved Chunks + Query â†’ Context Assembly â†’ LLM Prompt â†’ Generated Answer
```

### LangChain Components Used

#### 1. **Document Loaders** (`langchain_community.document_loaders`)
- **PyPDFLoader**: PDF text extraction
- **TextLoader**: Plain text files
- **Docx2txtLoader**: Microsoft Word documents
- **UnstructuredMarkdownLoader**: Markdown files

#### 2. **Text Splitters** (`langchain_text_splitters`)
- **RecursiveCharacterTextSplitter**: Intelligent chunking with overlap
  - Preserves document structure
  - Maintains line number tracking for citations
  - Configurable chunk size (1000) and overlap (200)

#### 3. **Vector Stores** (`langchain_community.vectorstores`)
- **MongoDBAtlasVectorSearch**: Native MongoDB integration
  - Custom embedding function wrapper
  - Vector search query construction
  - Metadata filtering capabilities

#### 4. **Retrievers** (`langchain.retrievers`)
- **VectorStoreRetriever**: Semantic similarity retrieval
  - Top-K retrieval (configurable)
  - Score-based ranking
  - Metadata preservation

#### 5. **LLM Integration** (`langchain.llms`)
- **Custom LLM Wrapper**: Llama 3.2 API integration
  - OpenAI-compatible endpoint support
  - Streaming capability
  - Token management

#### 6. **Chains** (`langchain.chains`)
- **RetrievalQA Chain**: End-to-end RAG pipeline
  - Automatic context injection
  - Prompt template management
  - Source attribution

### RAG Service Architecture

```python
class RAGService:
    """
    Orchestrates the complete RAG pipeline using LangChain components.
    """
    
    def __init__(self):
        # Initialize embedding service
        self.embedding_service = EmbeddingService()
        
        # Initialize vector store with MongoDB
        self.vector_store = MongoDBAtlasVectorSearch(
            embedding_function=self.embedding_service,
            collection=collection,
            index_name=index_name
        )
        
        # Create retriever
        self.retriever = self.vector_store.as_retriever(
            search_kwargs={"k": 5}  # Top 5 most relevant chunks
        )
        
        # Initialize LLM
        self.llm = CustomLlamaLLM(
            api_url=LLM_API_URL,
            api_key=LLM_API_KEY,
            model=LLM_MODEL
        )
        
        # Create RAG chain
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.retriever,
            return_source_documents=True
        )
    
    def query(self, question: str) -> QueryResponse:
        """
        Execute RAG pipeline:
        1. Retrieve relevant chunks
        2. Assemble context
        3. Generate answer with LLM
        4. Format response with sources
        """
        result = self.qa_chain.invoke({"query": question})
        
        return {
            "answer": result["result"],
            "sources": self._format_sources(result["source_documents"]),
            "query": question
        }
```

## ğŸ“Š Data Flow

### Document Ingestion Flow

```
1. File Upload (PDF/TXT/DOCX/MD)
   â†“
2. Document Processor
   â”œâ”€â†’ Extract text content
   â”œâ”€â†’ Validate file type & size
   â””â”€â†’ Return raw text
   â†“
3. Text Chunking (RecursiveCharacterTextSplitter)
   â”œâ”€â†’ Split into 1000-char chunks
   â”œâ”€â†’ 200-char overlap between chunks
   â”œâ”€â†’ Preserve line numbers (start, end)
   â””â”€â†’ Return: [(chunk_text, line_start, line_end), ...]
   â†“
4. Embedding Generation (SentenceTransformer)
   â”œâ”€â†’ Model: all-MiniLM-L6-v2
   â”œâ”€â†’ Dimension: 384
   â”œâ”€â†’ Batch processing for efficiency
   â””â”€â†’ Return: List[List[float]] (384-dim vectors)
   â†“
5. MongoDB Vector Store
   â”œâ”€â†’ Store chunks with embeddings
   â”œâ”€â†’ Metadata: file_name, line_start, line_end, content
   â”œâ”€â†’ Indexed for vector search
   â””â”€â†’ Ready for retrieval
```

### Query Processing Flow

```
1. User Query Input
   â†“
2. Query Embedding
   â”œâ”€â†’ Generate 384-dim vector for query
   â””â”€â†’ Same model as document embeddings
   â†“
3. Vector Search (MongoDB Atlas)
   â”œâ”€â†’ Cosine similarity search
   â”œâ”€â†’ Top-K retrieval (default: 5)
   â”œâ”€â†’ Score-based ranking
   â””â”€â†’ Return: [(chunk, score, metadata), ...]
   â†“
4. Context Assembly
   â”œâ”€â†’ Combine top-K chunks
   â”œâ”€â†’ Add source metadata
   â””â”€â†’ Format for LLM prompt
   â†“
5. LLM Generation (Llama 3.2)
   â”œâ”€â†’ Prompt: Query + Context + Instructions
   â”œâ”€â†’ Generate answer
   â””â”€â†’ Return: Generated text
   â†“
6. Response Formatting
   â”œâ”€â†’ Combine answer + sources
   â”œâ”€â†’ Include file names & line numbers
   â”œâ”€â†’ Add relevance scores
   â””â”€â†’ Return: QueryResponse
```

## ğŸ”§ Technical Implementation Details

### Vector Search Configuration

**MongoDB Atlas Vector Search Index:**
```json
{
  "mappings": {
    "dynamic": true,
    "fields": {
      "embedding": {
        "type": "knnVector",
        "dimensions": 384,
        "similarity": "cosine"
      }
    }
  }
}
```

**Search Pipeline:**
```python
pipeline = [
    {
        "$vectorSearch": {
            "index": "vector_index",
            "path": "embedding",
            "queryVector": query_embedding,
            "numCandidates": 10,
            "limit": 5
        }
    },
    {
        "$project": {
            "file_name": 1,
            "content": 1,
            "line_start": 1,
            "line_end": 1,
            "score": {"$meta": "vectorSearchScore"}
        }
    }
]
```

### Chunking Strategy

- **Method**: RecursiveCharacterTextSplitter
- **Chunk Size**: 1000 characters
- **Overlap**: 200 characters
- **Separators**: `["\n\n", "\n", ". ", " ", ""]`
- **Line Tracking**: Custom implementation to preserve line numbers

### Embedding Model

- **Model**: `all-MiniLM-L6-v2` (sentence-transformers)
- **Dimensions**: 384
- **Similarity**: Cosine
- **Advantages**: 
  - Fast inference
  - Good semantic understanding
  - No API costs
  - Privacy-preserving (local)

### LLM Integration

- **Model**: Llama 3.2
- **API Format**: OpenAI-compatible
- **Prompt Template**:
  ```
  Context: {retrieved_chunks}
  
  Question: {user_query}
  
  Instructions:
  - Answer based only on the provided context
  - Cite sources with file names and line numbers
  - If context is insufficient, state so clearly
  ```

## ğŸ“ Project Structure

```
mongo_rag/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                      # Flask application entry point
â”‚   â”œâ”€â”€ config.py                   # Environment configuration
â”‚   â”œâ”€â”€ models/                     # Data models
â”‚   â”‚   â”œâ”€â”€ document.py            # Document data structure
â”‚   â”‚   â””â”€â”€ query.py                # Query request/response models
â”‚   â”œâ”€â”€ services/                   # Core business logic
â”‚   â”‚   â”œâ”€â”€ document_processor.py  # Document loading & extraction
â”‚   â”‚   â”œâ”€â”€ embedding_service.py   # Embedding generation (sentence-transformers)
â”‚   â”‚   â”œâ”€â”€ vector_store.py        # MongoDB vector operations
â”‚   â”‚   â””â”€â”€ rag_service.py         # RAG pipeline orchestration (LangChain)
â”‚   â”œâ”€â”€ routes/                     # API endpoints
â”‚   â”‚   â”œâ”€â”€ upload.py               # File upload handler
â”‚   â”‚   â”œâ”€â”€ query.py               # Query processing endpoint
â”‚   â”‚   â””â”€â”€ health.py               # Health check endpoint
â”‚   â””â”€â”€ utils/                      # Utility functions
â”‚       â”œâ”€â”€ file_validator.py       # File validation
â”‚       â””â”€â”€ chunking.py             # Text chunking with line tracking
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/             # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ FileUpload.tsx     # Document upload UI
â”‚   â”‚   â”‚   â”œâ”€â”€ QueryInput.tsx      # Query input with suggestions
â”‚   â”‚   â”‚   â”œâ”€â”€ ResponseDisplay.tsx # Answer display
â”‚   â”‚   â”‚   â””â”€â”€ SourceReference.tsx # Source citation component
â”‚   â”‚   â”œâ”€â”€ api.ts                  # API client
â”‚   â”‚   â”œâ”€â”€ types.ts                # TypeScript definitions
â”‚   â”‚   â””â”€â”€ App.tsx                 # Main application component
â”‚   â””â”€â”€ package.json               # Frontend dependencies
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Node.js 18+
- MongoDB Atlas account
- Llama 3.2 API access

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/HARISRUJAN/Atlas-RAG-Assistant.git
cd Atlas-RAG-Assistant
```

2. **Backend Setup**
```bash
# Create virtual environment
python -m venv rag_env
rag_env\Scripts\activate  # Windows
source rag_env/bin/activate  # macOS/Linux

# Install dependencies
pip install -r requirements.txt
```

3. **Frontend Setup**
```bash
cd frontend
npm install
```

4. **Environment Configuration**
Create `.env` file:
```env
MONGODB_URI=mongodb+srv://user:pass@cluster.mongodb.net/
MONGODB_DATABASE_NAME=rag_database
MONGODB_COLLECTION_NAME=documents
MONGODB_VECTOR_INDEX_NAME=vector_index

LLM_API_URL=https://your-llama-api.com/v1/completions
LLM_API_KEY=your-api-key
LLM_MODEL=llama3.2:latest

EMBEDDING_MODEL=all-MiniLM-L6-v2
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

5. **Create MongoDB Vector Search Index**
- Go to MongoDB Atlas â†’ Search â†’ Create Index
- Database: `rag_database`, Collection: `documents`
- Use the JSON configuration shown in "Vector Search Configuration" above

6. **Run the Application**
```bash
# Backend (from project root)
python -m backend.app

# Frontend (from frontend directory)
npm run dev
```

Access the application at `http://localhost:5173`

## ğŸ“ Key LangChain Concepts Demonstrated

1. **Document Processing Pipeline**
   - Multi-format document loading
   - Intelligent text splitting
   - Metadata preservation

2. **Vector Store Integration**
   - Custom embedding function
   - MongoDB Atlas native support
   - Efficient similarity search

3. **Retrieval Strategies**
   - Top-K retrieval
   - Score-based ranking
   - Metadata filtering

4. **Chain Composition**
   - RetrievalQA chain
   - Context injection
   - Prompt templating

5. **LLM Abstraction**
   - Custom LLM wrapper
   - API compatibility layer
   - Streaming support

## ğŸ“ˆ Performance Optimizations

- **Batch Embedding**: Process multiple chunks simultaneously
- **Vector Index**: MongoDB Atlas optimized for similarity search
- **Chunk Overlap**: Prevents context loss at boundaries
- **Line Number Tracking**: Enables precise source citations
- **Top-K Retrieval**: Balances relevance and performance

## ğŸ”’ Security Considerations

- Environment variables for sensitive data
- File type validation
- Size limits (10MB max)
- No API keys in codebase
- Local embedding generation (privacy-preserving)

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| **Backend Framework** | Flask (Python) |
| **RAG Framework** | LangChain |
| **Vector Database** | MongoDB Atlas Vector Search |
| **Embeddings** | sentence-transformers (all-MiniLM-L6-v2) |
| **LLM** | Llama 3.2 |
| **Frontend** | React + TypeScript + Vite |
| **Styling** | Tailwind CSS |
| **Document Processing** | PyPDF2, python-docx, markdown |

## ğŸ“ License

MIT
